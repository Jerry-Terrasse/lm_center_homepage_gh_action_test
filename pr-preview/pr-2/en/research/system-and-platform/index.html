<!doctype html><html lang=en dir=ltr data-wc-theme-default=system><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 0.3.1"><meta name=author content="Tong Lu"><meta name=description content="The Large Model Systems and Platforms Research Group focuses on the construction of systems based on large models, large-scale training/inference deployment, and the application of large models. The group conducts research to address key challenges in efficient training, deployment, and the integration of domain knowledge into large models. In terms of applications, the group has a strong focus on reasoning tasks such as Automated Theorem Proving (ATP). In undergraduate education, the group offers courses on large model development, training students to build large models from scratch.
"><link rel=alternate hreflang=zh href=https://Jerry-Terrasse.github.io/lm_center_homepage_gh_action_test/pr-preview/pr-2/research/system-and-platform/><link rel=alternate hreflang=en href=https://Jerry-Terrasse.github.io/lm_center_homepage_gh_action_test/pr-preview/pr-2/en/research/system-and-platform/><link rel=stylesheet href=/lm_center_homepage_gh_action_test/pr-preview/pr-2/css/themes/emerald.min.css><link href=/lm_center_homepage_gh_action_test/pr-preview/pr-2/dist/wc.min.css rel=stylesheet><link href=/lm_center_homepage_gh_action_test/pr-preview/pr-2/css/custom.min.97983a506d4b89401c7dc53cfb95c5059b49f13efdb74d01c3d038d88c173bcc.css rel=stylesheet><script>window.hbb={defaultTheme:document.documentElement.dataset.wcThemeDefault,setDarkTheme:()=>{document.documentElement.classList.add("dark"),document.documentElement.style.colorScheme="dark"},setLightTheme:()=>{document.documentElement.classList.remove("dark"),document.documentElement.style.colorScheme="light"}},console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`),"wc-color-theme"in localStorage?localStorage.getItem("wc-color-theme")==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme():(window.hbb.defaultTheme==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme(),window.hbb.defaultTheme==="system"&&(window.matchMedia("(prefers-color-scheme: dark)").matches?window.hbb.setDarkTheme():window.hbb.setLightTheme()))</script><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll("li input[type='checkbox'][disabled]");e.forEach(e=>{e.parentElement.parentElement.classList.add("task-list")});const t=document.querySelectorAll(".task-list li");t.forEach(e=>{let t=Array.from(e.childNodes).filter(e=>e.nodeType===3&&e.textContent.trim().length>1);if(t.length>0){const n=document.createElement("label");t[0].after(n),n.appendChild(e.querySelector("input[type='checkbox']")),n.appendChild(t[0])}})})</script><link rel=icon type=image/png href=/lm_center_homepage_gh_action_test/pr-preview/pr-2/media/icon_hu3247630877640252165.png><link rel=apple-touch-icon type=image/png href=/lm_center_homepage_gh_action_test/pr-preview/pr-2/media/icon_hu4166356570829923896.png><link rel=canonical href=https://Jerry-Terrasse.github.io/lm_center_homepage_gh_action_test/pr-preview/pr-2/en/research/system-and-platform/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@GetResearchDev"><meta property="twitter:creator" content="@GetResearchDev"><meta property="og:site_name" content="Hugo Academic CV Theme"><meta property="og:url" content="https://Jerry-Terrasse.github.io/lm_center_homepage_gh_action_test/pr-preview/pr-2/en/research/system-and-platform/"><meta property="og:title" content="Large Model Systems and Platforms Research Group | Hugo Academic CV Theme"><meta property="og:description" content="The Large Model Systems and Platforms Research Group focuses on the construction of systems based on large models, large-scale training/inference deployment, and the application of large models. The group conducts research to address key challenges in efficient training, deployment, and the integration of domain knowledge into large models. In terms of applications, the group has a strong focus on reasoning tasks such as Automated Theorem Proving (ATP). In undergraduate education, the group offers courses on large model development, training students to build large models from scratch."><meta property="og:image" content="https://Jerry-Terrasse.github.io/lm_center_homepage_gh_action_test/pr-preview/pr-2/research/system-and-platform/featured.jpg"><meta property="twitter:image" content="https://Jerry-Terrasse.github.io/lm_center_homepage_gh_action_test/pr-preview/pr-2/research/system-and-platform/featured.jpg"><meta property="og:locale" content="en"><meta property="article:published_time" content="1010-01-01T00:00:00+00:00"><meta property="article:modified_time" content="1010-01-01T00:00:00+00:00"><title>Large Model Systems and Platforms Research Group | Hugo Academic CV Theme</title><style>@font-face{font-family:inter var;font-style:normal;font-weight:100 900;font-display:swap;src:url(/lm_center_homepage_gh_action_test/pr-preview/pr-2/dist/font/Inter.var.woff2)format(woff2)}</style><link type=text/css rel=stylesheet href=/lm_center_homepage_gh_action_test/pr-preview/pr-2/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css integrity="sha256-vnZutBkxehTsdp0hbpd5v+jzc3yA54D0ug2vtXpBpII="><script src=/lm_center_homepage_gh_action_test/pr-preview/pr-2/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script><script>window.hbb.pagefind={baseUrl:"/lm_center_homepage_gh_action_test/pr-preview/pr-2/"}</script><style>html.dark{--pagefind-ui-primary:#eeeeee;--pagefind-ui-text:#eeeeee;--pagefind-ui-background:#152028;--pagefind-ui-border:#152028;--pagefind-ui-tag:#152028}</style><script>window.addEventListener("DOMContentLoaded",e=>{new PagefindUI({element:"#search",showSubResults:!0,baseUrl:window.hbb.pagefind.baseUrl,bundlePath:window.hbb.pagefind.baseUrl+"pagefind/"})}),document.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("search"),t=document.getElementById("search_toggle");t&&t.addEventListener("click",()=>{if(e.classList.toggle("hidden"),e.querySelector("input").value="",e.querySelector("input").focus(),!e.classList.contains("hidden")){let t=document.querySelector(".pagefind-ui__search-clear");t&&!t.hasAttribute("listenerOnClick")&&(t.setAttribute("listenerOnClick","true"),t.addEventListener("click",()=>{e.classList.toggle("hidden")}))}})})</script><link type=text/css rel=stylesheet href=/lm_center_homepage_gh_action_test/pr-preview/pr-2/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM="><script defer src=/lm_center_homepage_gh_action_test/pr-preview/pr-2/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js integrity="sha256-3ISyluw+iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script><script defer src=/lm_center_homepage_gh_action_test/pr-preview/pr-2/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script><script defer src=/lm_center_homepage_gh_action_test/pr-preview/pr-2/js/hugo-blox-en.min.8c8ea06bd0420f5067e52fa727b9f92303757322ba4431774153d59a9735eadb.js integrity="sha256-jI6ga9BCD1Bn5S+nJ7n5IwN1cyK6RDF3QVPVmpc16ts="></script><script async defer src=https://buttons.github.io/buttons.js></script></head><body class="dark:bg-hb-dark dark:text-white page-wrapper" id=top><div id=page-bg></div><div class="page-header sticky top-0 z-30"><header id=site-header class=header><nav class="navbar px-3 flex justify-left"><div class="order-0 h-100"><a class=navbar-brand href=/lm_center_homepage_gh_action_test/pr-preview/pr-2/en/ title="Hugo Academic CV Theme"><img fetchpriority=high decoding=async width=36 height=36 src=/lm_center_homepage_gh_action_test/pr-preview/pr-2/media/icons/nju-emblem-cs_hu2207772332457637832.webp alt="Hugo Academic CV Theme">
<span class=max-lg:hidden>大模型研究协同创新中心</span></a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Open Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Close Menu</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left"><li class=nav-item><a class=nav-link href=/en/lm_center_homepage_gh_action_test/pr-preview/pr-2/>Home</a></li><li class=nav-item><a class=nav-link href=/en/lm_center_homepage_gh_action_test/pr-preview/pr-2/post>News</a></li><li class=nav-item><a class=nav-link href=/en/lm_center_homepage_gh_action_test/pr-preview/pr-2/research>Research</a></li><li class=nav-item><a class=nav-link href=/lm_center_homepage_gh_action_test/pr-preview/pr-2/en/publication/>Publication</a></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="text-black hover:text-primary inline-block px-3 text-xl dark:text-white" id=search_toggle><svg height="16" width="16" viewBox="0 0 512 512" fill="currentcolor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg></button><div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><button class="theme-toggle mt-1" accesskey=t title=appearance><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:hidden"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:block [&:not(dark)]:hidden"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div><div class="pl-1 mr-5 text-gray-600 hover:text-gray-800 dark:text-gray-400 dark:hover:text-gray-200
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><div class="flex justify-items-start"><button title=Languages data-state=closed data-hb-language-chooser class="grow h-7 rounded-md px-2 text-left text-xs font-medium text-gray-600 transition-colors dark:text-gray-400 hover:bg-gray-100 hover:text-gray-900 dark:hover:bg-primary-100/5 dark:hover:text-gray-50" type=button aria-label=Languages><div class="flex items-center gap-2 capitalize"><svg height="18" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m10.5 21 5.25-11.25L21 21m-9-3h7.5M3 5.621a48.474 48.474.0 016-.371m0 0c1.12.0 2.233.038 3.334.114M9 5.25V3m3.334 2.364C11.176 10.658 7.69 15.08 3 17.502m9.334-12.138A47.63 47.63.0 0115 5.621m-4.589 8.495A18.023 18.023.0 016.584 8.314"/></svg><span>English</span></div></button><ul class="fixed m-0 min-w-[100px] hidden z-20 max-h-64 overflow-auto rounded-md ring-1 ring-black/5 bg-white py-1 text-sm shadow-lg dark:ring-white/20 dark:bg-neutral-800" style="inset:auto auto 0 0"><li class="flex flex-col"><a href=https://Jerry-Terrasse.github.io/lm_center_homepage_gh_action_test/pr-preview/pr-2/research/system-and-platform/ class="relative cursor-pointer text-gray-800 dark:text-gray-100 hover:bg-primary-50 hover:text-primary-600 hover:dark:bg-primary-500/10 hover:dark:text-primary-200 whitespace-nowrap py-1.5 transition-colors ltr:pl-3 ltr:pr-9 rtl:pr-3 rtl:pl-9">中文 (简体)</a></li></ul></div></div></div></nav></header><div id=search class="hidden p-3 bg-light dark:bg-dark"></div></div><div class=page-body><section id=section-markdown class="relative hbb-section blox-markdown" style="padding:1rem 0"><div class=home-section-bg></div><div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6"><div class="prose prose-slate dark:prose-invert max-w-prose"><h3 class="mb-6 font-bold" style=margin-bottom:1.5rem></h3></div><div class="prose prose-slate dark:prose-invert max-w-prose"><h2 id=large-model-systems-and-platforms-research-group>Large Model Systems and Platforms Research Group</h2></div></div></section><section id=members class="relative hbb-section blox-collection" style="padding:1rem 0"><div class=home-section-bg></div><div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6 md:px-0"><div class="prose prose-slate dark:prose-invert max-w-prose"><h3 class=font-bold>Team Members</h3></div></div><div class="flex flex-col items-center px-6"><div class="container px-8 mx-auto xl:px-5 py-5 lg:py-8 max-w-[500px] justify-center"><div class="grid gap-10 md:grid-cols-1 lg:gap-10"><div class="resume-biography flex justify-center items-center flex-col"><div class="avatar-wrapper mt-5"><img class="avatar rounded-full bg-white dark:bg-gray-800 p-1" src=/lm_center_homepage_gh_action_test/pr-preview/pr-2/author/%E6%9D%8E%E6%AD%A6%E5%86%9B/avatar_hu10238422404925903220.png alt="Wujun Li" width=150 height=150></div><div class="portrait-title dark:text-white" style=text-align:center><div class="text-2xl font-bold mb-2 mt-6"><a href=https://cs.nju.edu.cn/lwj/ target=_blank rel=noopener>Wujun Li</a></div><h3 class="font-semibold mb-1"><a href=https://www.nju.edu.cn>Nanjing University  School of Computer Science</a></h3></div><ul class="network-icon dark:text-zinc-100"><li class="mt-1 mb-1"><a href=mailto:liwujun@nju.edu.cn aria-label=at-symbol data-toggle=tooltip data-placement=top title="E-mail Me"><svg style="height:1.5rem" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5.0 11-9 0 4.5 4.5.0 019 0zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 10-2.636 6.364M16.5 12V8.25"/></svg></a></li><li class="mt-1 mb-1"><a href=https://dblp.org/pid/26/988.html target=_blank rel=noopener aria-label=custom/dblp><!doctype html><svg style="height:1.5rem" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" width="28" height="28" viewBox="0 0 28 28" enable-background="new 0 0 28 28"><image id="image0" width="28" height="28" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAMAAABF0y+mAAAAIGNIUk0AAHomAACAhAAA+gAAAIDo AAB1MAAA6mAAADqYAAAXcJy6UTwAAAMAUExURQAAACWBxCWCxCeAxB6HyyJ/xSWAwyWAxSOFxP/V AApcqf/////WACWBxQlaqP/TABpdj//UACaBxCSBxP/XAP/EACaDw+W0FCWCwjuHsPzAACWBw//f AP//ABttoyWBxCWBxCWBxCWBxCWBxCWBxCWBxCWBxCWBxCWBxCWBxCWBxCWBxCWBxCWBxCWBxCWB xCWBxCWBxCWBxCWBxCWBxCaCxSiFxyWBxBNosgdYpv/VAP/VAP/VACWBxCWBxCWAxB13vBBkrwRU owBPnwBPn//VAP/VAP/VAP/VAP/VACWCxCR/wxx0uw5hrQNTogBOov/uAP/VAP/VAP/VAP/VAP/V ACWBxCWBxBBlryVgd+7MB//VAP/VAP/VACWBxCWBxCWCxBNpsgJQmiVdV05vK93BB//VACWBxCWB xB53vQNUogNQlyRaPjdiJ7WqEP/VAP/VACWBxCWBxBpyuQhZpwBPnxhWVyVZKHiEGv3OAP/VAP/V AP/VACWAxSWBxCJ9wRNosg5TcyVZKktqIuq0A/7GAP7SAP/VAP/VACWBxB13vQxfrAhSjCNYMjFe J8mjCvy+AP3LAP/UAP/VACWBxCWBxCR/wxpwsiZgSSZZKJqOE/u6APzCAP7QAP/VAP/VACWBxCWC xjt8cztpKm58Hfi5Afy/AP/SAP/VAP/VACWBxDWAll6BL6CXGPm5AP7MAP/VAP/VAP/VACWBxCWB xCSBxj2Go8+rEfu7AP3IAP/WAP/VACWBxCWBxCWBxCWBxCOAxrapR/67APu8APzEAP7OAP/UAP/W ACWBxCWBxCWBxCWBxCWBxCeFxfu6APu9AP3GAP/VAP/VACWBxCWBxCWBxPy/AP3HAP/VAP/WAP/V AP/VAP/VAP/XAP/VAP/VAP/VAP/VAP/VAP/VAP/VAP/VAP/VAP/VAP/VAP/VAP/VAP/VAP/VACWB xCWCxB95viaCxf/VAABOnwBPnwJRoQBOngFPm//WAP/XAABOoP67APu6APu5ACSBxve5Af7RABbu ZnwAAADtdFJOUwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEEQ2aZgz+sTpc/emFGyrn zVgIC773piyGxRskcDVM+f76+fz8VwIuiN2QHNv8+vyRBTqX5sYODlfc1art7DEiufj6/Pb7/mU3 8/v8/Pf596ABFND6+v76/vX5/tMWAZz8+fr++/r5+/M+Yfv6+f39+/v5/XUu6v339v76/vr7sAXD /vb59/36/N8ejP379v3796sUUfrpreX+zU0MI+HikTQMtvz5/r8JB7bYgSkBffz56i1OcB8z3mFC vv2bARJyzyyf9PIGUchzF4GJA6iXOowAAAABYktHRAsf18TAAAAACXBIWXMAAA3XAAAN1wFCKJt4 AAAAB3RJTUUH6QIKCSUXjBM9QQAAAfpJREFUKM9jYGBglFdgZGRkYsAKGBWVlFWA0sxYJVmUVNXU WRnZGLFJMmpovtXS1gFqxpRmZ9TVe/v2rb6BISMHSJYTBBBajYyBkm9NTM24uIFyPCBJHpgkL6M5 UO7d2/cWlnxAcX4raxuEVkZGW7sP9g6OTs4uApyubu4eHz0FYTqFhBm9vH18/T59/uwfEBgU/PHj x5BQTrjGsPCIL18/f/78LTIq+iMIxMRyikAl4+ITEkFySckpqd/Bkh/TOAWhkukZ7zKzgLLZObl5 PyCS+QVQcxkLi96qFpeUfi4tK6+orAJLVtfAJGvrgB6pb8j62djU3NLaBpJs74D6lJexUxXoT9Wu 7p7evv5fEyZOAspO5hSFSk6ZCgqhd9Omz5g569fv2XPmfvw4bz7MXMUFb8Fg4aLFS5b+/v1n2fKP K1ZywoJvFUTy7+o1a9f9/v179voNGzfBdDJu3gKW3Lpt+z+g3O8dO3fthoWuGOOevUCpffsPHDwE lDp85Oix44h4YTxx8u2p02fOComf+33+wtyqjxcvwYKPgUGC8fKVq8A0Jil17fp/UChU3YAFH1Cn NCMkjcjI3rx1GxQId+4i0gIbEMhBUsi9+yuAkg82ISThAJRCHj56/PFJICKhoEo/ffb84wtOLFoh ml++ev0GqyRYumCTK1ASAGVb+aortizsAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDI1LTAyLTEwVDA5 OjM3OjIzKzAwOjAwO2E28AAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyNS0wMi0xMFQwOTozNzoyMysw MDowMEo8jkwAAAAodEVYdGRhdGU6dGltZXN0YW1wADIwMjUtMDItMTBUMDk6Mzc6MjMrMDA6MDAd Ka+TAAAAAElFTkSuQmCC"/></svg></a></li></ul></div></div></div></div></section><section id=section-markdown class="relative hbb-section blox-markdown" style="padding:1rem 0"><div class=home-section-bg></div><div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6"><div class="prose prose-slate dark:prose-invert max-w-prose"><h3 class="mb-6 font-bold" style=margin-bottom:1.5rem>Research Overview</h3></div><div class="prose prose-slate dark:prose-invert max-w-prose"><p>The high costs (GPU hardware, electricity, etc.) have become a major hurdle for the sustainable development of artificial intelligence, especially for large models. Nanjing University’s School of Computer Science has developed efficient training and inference algorithms that accelerate large model processes through algorithm innovation, reducing costs or enabling the training of larger, more accurate models with the same resources. Furthermore, by integrating these innovations, they have built a training and inference system and platform that supports high accuracy with low-cost deployment. Representative achievements include:</p><h3 id=achievement-1-efficient-distributed-training-algorithms-and-platform>Achievement 1: Efficient Distributed Training Algorithms and Platform</h3><p><figure><div class="flex justify-center"><div class=w-100><img src=/research/system-and-platform/uniap.png alt=UniAP loading=lazy data-zoomable></div></div></figure></p><p>Training large models often requires multi-machine, multi-GPU distributed training. Distributed training poses significant challenges—in our experiments, 64%–87% of runs fail due to suboptimal hyperparameter setups (e.g., model partitioning, data distribution). Moreover, when training is slow, many only consider scaling hardware, overlooking the impact of distributed training algorithms that can significantly boost computational efficiency, often several times or even dozens of times faster. We propose a series of efficient algorithms—including communication optimization, asynchronous methods, fault tolerance, and automatic parallelism—and developed UniAP, the first platform to jointly optimize intra-layer and inter-layer parallel strategies. Given a model and hardware configuration, UniAP can automatically search for the optimal distributed training strategy, achieving speedups of up to 9x under certain conditions, while addressing issues due to improper hyperparameter settings. UniAP has also been adapted to domestic AI hardware.</p><p><strong>Related Papers:</strong></p><ul><li>Shen-Yi Zhao, Chang-Wei Shi, Yin-Peng Xie, Wu-Jun Li, <a href=https://arxiv.org/abs/2007.13985 target=_blank>Stochastic Normalized Gradient Descent with Momentum for Large-Batch Training</a>, SCIENCE CHINA Information Sciences (SCIS) 2024.</li><li>Hao Lin, Ke Wu, Jie Li, Jun Li, Wu-Jun Li, <a href=https://arxiv.org/abs/2307.16375 target=_blank>UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming</a>, arXiv 2023.</li><li>Chang-Wei Shi, Yi-Rui Yang, Wu-Jun Li, <a href=https://arxiv.org/abs/2407.19234 target=_blank>Ordered Momentum for Asynchronous SGD</a>, NeurIPS 2024.</li><li>Yi-Rui Yang, Chang-Wei Shi, Wu-Jun Li, <a href="https://openreview.net/forum?id=wriKDQqiOQ" target=_blank>On the Effect of Batch Size in Byzantine-Robust Distributed Learning</a>, ICLR 2024.</li></ul><h3 id=achievement-2-efficient-training-algorithms-based-on-continual-learning>Achievement 2: Efficient Training Algorithms Based on Continual Learning</h3><p><figure><div class="flex justify-center"><div class=w-100><img src=/research/system-and-platform/inflora.png alt=InfLoRA loading=lazy data-zoomable></div></div></figure></p><p>Continual learning aims to enable models to constantly learn new tasks while retaining previous knowledge, which is crucial for efficiently training large models. Typically, training large models requires massive GPU clusters and huge datasets, incurring high costs. With continual learning, new versions of large models can be incrementally built on previous ones without re-accessing all historical data, substantially reducing training overhead. However, existing large models often suffer from “catastrophic forgetting” where new task training compromises previous performance. To overcome this, models must balance stability (retaining old skills) and plasticity (learning new tasks). We propose InfLoRA, a novel method that injects a low-rank branch into pre-trained weights; our theoretical analysis shows that fine-tuning the low-rank branch is equivalent to adjusting weights within a subspace spanned by the low-rank matrices. This carefully designed subspace avoids interfering with previous knowledge, striking an effective balance to improve overall model accuracy. InfLoRA is the first approach to bridge LoRA-based tuning with full-parameter fine-tuning for overcoming forgetting.</p><p><strong>Related Papers:</strong></p><ul><li>Yan-Shuo Liang, Wu-Jun Li, <a href=https://openaccess.thecvf.com/content/CVPR2024/html/Liang_InfLoRA_Interference-Free_Low-Rank_Adaptation_for_Continual_Learning_CVPR_2024_paper.html target=_blank>InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning</a>, CVPR 2024.</li></ul><h3 id=achievement-3-efficient-inference-algorithms-and-platform>Achievement 3: Efficient Inference Algorithms and Platform</h3><p><figure><div class="flex justify-center"><div class=w-100><img src=/research/system-and-platform/pipo_arch.png alt="PIPO Arch" loading=lazy data-zoomable></div></div></figure></p><p><figure><div class="flex justify-center"><div class=w-100><img src=/research/system-and-platform/pipo_bench.png alt="PIPO Benchmark" loading=lazy data-zoomable></div></div></figure>Experimental Setup: Lenovo Thinkbook laptop (RTX3060 GPU with 6GB VRAM, Intel i7-11800H @ 2.30GHz, 16GB RAM, 1TB SSD); model weights quantized to INT4.</p><p>As large models evolve, the demand for inference and deployment hardware rises sharply. To address this, we innovated in both model compression and system architecture to design an efficient inference algorithm and platform. In terms of compression, our LCQ (Low-rank Codebook Quantization) allows the quantization dictionary to have a rank greater than one—reducing quantization loss compared to traditional approaches with a rank of one. Regarding system architecture, our offloading method stores parts of the model in CPU memory or on disk, enabling the inference of models that exceed the GPU&rsquo;s memory capacity. Existing offloading frameworks (like FlexGen) suffer from low concurrency and suboptimal disk utilization, leading to poor GPU usage. Our novel Pipelined Offloading (PIPO) framework automatically determines the optimal offloading strategy based on model and hardware specifications, complemented by data transfer optimizations and custom CUDA kernel modifications to boost inference throughput. Experiments show that while conventional methods reach less than 40% GPU utilization, PIPO can push it above 90%, with inference speeds up to 3.1 times faster.</p><p><strong>Related Papers:</strong></p><ul><li>Wen-Pu Cai, Ming-Yang Li, Wu-Jun Li, <a href=https://arxiv.org/abs/2405.20973 target=_blank>LCQ: Low-Rank Codebook based Quantization for Large Language Models</a>, arXiv 2024.<div class="flex flex-wrap space-x-3"><a><img src=/images/links.svg class=inline-block style=height:1.25em></a>
<a href=https://github.com/liangyanshuo/InfLoRA target=_blank><img src=/images/github.svg class=inline-block style=height:1.5em></img></a></div></li><li>Yangyijian Liu, Jun Li, Wu-Jun Li, <a href=placeholder target=_blank>PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices</a>, Submitted, 2025.</li></ul></div></div></section></div><div class=page-footer><footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200"><div class="mx-auto flex gap-3 py-2 px-4"><div class=font-bold><svg class="inline-block pr-1" style="height:1em" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M12 21a9.004 9.004.0 008.716-6.747M12 21a9.004 9.004.0 01-8.716-6.747M12 21c2.485.0 4.5-4.03 4.5-9S14.485 3 12 3m0 18c-2.485.0-4.5-4.03-4.5-9S9.515 3 12 3m0 0a8.997 8.997.0 017.843 4.582M12 3A8.997 8.997.0 004.157 7.582m15.686.0A11.953 11.953.0 0112 10.5c-2.998.0-5.74-1.1-7.843-2.918m15.686.0A8.959 8.959.0 0121 12c0 .778-.099 1.533-.284 2.253m0 0A17.919 17.919.0 0112 16.5a17.92 17.92.0 01-8.716-2.247m0 0A9.015 9.015.0 013 12c0-1.605.42-3.113 1.157-4.418"/></svg>Languages:</div><div class=font-bold>English</div><div><a href=https://Jerry-Terrasse.github.io/lm_center_homepage_gh_action_test/pr-preview/pr-2/research/system-and-platform/>中文 (简体)</a></div></div><p class="powered-by text-center">© 2025 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class="powered-by text-center">Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div><script src=https://cdn.staticfile.net/typed.js/2.1.0/typed.umd.min.js></script><script>var typed,element=document.getElementById("typed"),text=element?element.innerText:"";element.innerHTML="",typed=new Typed("#typed",{strings:[text],typeSpeed:100,showCursor:!1})</script></body></html>