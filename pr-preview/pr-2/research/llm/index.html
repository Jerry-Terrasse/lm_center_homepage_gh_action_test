<!doctype html><html lang=zh dir=ltr data-wc-theme-default=system><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 0.3.1"><meta name=author content="大模型研究协同中心"><meta name=description content="大语言模型：人工智能语言理解与生成的新纪元 大语言模型（Large Language Models, LLMs） 是近年来人工智能领域最具突破性的技术之一。它们通过海量数据训练，能够理解和生成自然语言，在文本生成、翻译、问答等任务中展现出接近人类水平的能力，开启了人工智能语言理解与生成的新纪元。
核心技术突破 大语言模型的成功得益于以下关键技术的突破：
总而言之，大语言模型正在深刻改变我们与机器的交互方式，并为各行各业带来前所未有的机遇。 随着技术的不断进步和应用场景的不断拓展，它们将继续推动人工智能语言理解与生成领域的发展，为人类社会创造更多价值。
"><link rel=alternate hreflang=en href=/en/research/llm/><link rel=alternate hreflang=zh href=/research/llm/><link rel=stylesheet href=/css/themes/emerald.min.css><link href=/dist/wc.min.css rel=stylesheet><link href=/css/custom.min.97983a506d4b89401c7dc53cfb95c5059b49f13efdb74d01c3d038d88c173bcc.css rel=stylesheet><script>window.hbb={defaultTheme:document.documentElement.dataset.wcThemeDefault,setDarkTheme:()=>{document.documentElement.classList.add("dark"),document.documentElement.style.colorScheme="dark"},setLightTheme:()=>{document.documentElement.classList.remove("dark"),document.documentElement.style.colorScheme="light"}},console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`),"wc-color-theme"in localStorage?localStorage.getItem("wc-color-theme")==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme():(window.hbb.defaultTheme==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme(),window.hbb.defaultTheme==="system"&&(window.matchMedia("(prefers-color-scheme: dark)").matches?window.hbb.setDarkTheme():window.hbb.setLightTheme()))</script><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll("li input[type='checkbox'][disabled]");e.forEach(e=>{e.parentElement.parentElement.classList.add("task-list")});const t=document.querySelectorAll(".task-list li");t.forEach(e=>{let t=Array.from(e.childNodes).filter(e=>e.nodeType===3&&e.textContent.trim().length>1);if(t.length>0){const n=document.createElement("label");t[0].after(n),n.appendChild(e.querySelector("input[type='checkbox']")),n.appendChild(t[0])}})})</script><link rel=icon type=image/png href=/media/icon_hu3247630877640252165.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu4166356570829923896.png><link rel=canonical href=/research/llm/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@GetResearchDev"><meta property="twitter:creator" content="@GetResearchDev"><meta property="og:site_name" content="Hugo Academic CV Theme"><meta property="og:url" content="/research/llm/"><meta property="og:title" content="大语言模型研究小组 | Hugo Academic CV Theme"><meta property="og:description" content="大语言模型：人工智能语言理解与生成的新纪元
大语言模型（Large Language Models, LLMs） 是近年来人工智能领域最具突破性的技术之一。它们通过海量数据训练，能够理解和生成自然语言，在文本生成、翻译、问答等任务中展现出接近人类水平的能力，开启了人工智能语言理解与生成的新纪元。
核心技术突破
大语言模型的成功得益于以下关键技术的突破：
总而言之，大语言模型正在深刻改变我们与机器的交互方式，并为各行各业带来前所未有的机遇。 随着技术的不断进步和应用场景的不断拓展，它们将继续推动人工智能语言理解与生成领域的发展，为人类社会创造更多价值。"><meta property="og:image" content="/research/llm/featured.jpg"><meta property="twitter:image" content="/research/llm/featured.jpg"><meta property="og:locale" content="zh"><meta property="article:published_time" content="1030-01-01T00:00:00+00:00"><meta property="article:modified_time" content="1030-01-01T00:00:00+00:00"><title>大语言模型研究小组 | Hugo Academic CV Theme</title><style>@font-face{font-family:inter var;font-style:normal;font-weight:100 900;font-display:swap;src:url(/dist/font/Inter.var.woff2)format(woff2)}</style><link type=text/css rel=stylesheet href=/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css integrity="sha256-vnZutBkxehTsdp0hbpd5v+jzc3yA54D0ug2vtXpBpII="><script src=/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script><script>window.hbb.pagefind={baseUrl:"/"}</script><style>html.dark{--pagefind-ui-primary:#eeeeee;--pagefind-ui-text:#eeeeee;--pagefind-ui-background:#152028;--pagefind-ui-border:#152028;--pagefind-ui-tag:#152028}</style><script>window.addEventListener("DOMContentLoaded",e=>{new PagefindUI({element:"#search",showSubResults:!0,baseUrl:window.hbb.pagefind.baseUrl,bundlePath:window.hbb.pagefind.baseUrl+"pagefind/"})}),document.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("search"),t=document.getElementById("search_toggle");t&&t.addEventListener("click",()=>{if(e.classList.toggle("hidden"),e.querySelector("input").value="",e.querySelector("input").focus(),!e.classList.contains("hidden")){let t=document.querySelector(".pagefind-ui__search-clear");t&&!t.hasAttribute("listenerOnClick")&&(t.setAttribute("listenerOnClick","true"),t.addEventListener("click",()=>{e.classList.toggle("hidden")}))}})})</script><link type=text/css rel=stylesheet href=/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM="><script defer src=/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js integrity="sha256-3ISyluw+iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script><script defer src=/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script><script defer src=/js/hugo-blox-zh.min.fd6dff43d6d3c4b67a94933185c6049c55f73fec192e44b3fe29bcdce8cd747a.js integrity="sha256-/W3/Q9bTxLZ6lJMxhcYEnFX3P+wZLkSz/im83OjNdHo="></script><script async defer src=https://buttons.github.io/buttons.js></script></head><body class="dark:bg-hb-dark dark:text-white page-wrapper" id=top><div id=page-bg></div><div class="page-header sticky top-0 z-30"><header id=site-header class=header><nav class="navbar px-3 flex justify-left"><div class="order-0 h-100"><a class=navbar-brand href=/ title="Hugo Academic CV Theme"><img fetchpriority=high decoding=async width=36 height=36 src=/media/icons/nju-emblem-cs_hu2207772332457637832.webp alt="Hugo Academic CV Theme">
<span class=max-lg:hidden>大模型研究协同创新中心</span></a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Open Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Close Menu</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left"><li class=nav-item><a class=nav-link href=/>主页</a></li><li class=nav-item><a class=nav-link href=/post>新闻</a></li><li class=nav-item><a class=nav-link href=/research>研究小组</a></li><li class=nav-item><a class=nav-link href=/publication/>出版物</a></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="text-black hover:text-primary inline-block px-3 text-xl dark:text-white" id=search_toggle><svg height="16" width="16" viewBox="0 0 512 512" fill="currentcolor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg></button><div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><button class="theme-toggle mt-1" accesskey=t title=appearance><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:hidden"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:block [&:not(dark)]:hidden"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div><div class="pl-1 mr-5 text-gray-600 hover:text-gray-800 dark:text-gray-400 dark:hover:text-gray-200
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><div class="flex justify-items-start"><button title=语言 data-state=closed data-hb-language-chooser class="grow h-7 rounded-md px-2 text-left text-xs font-medium text-gray-600 transition-colors dark:text-gray-400 hover:bg-gray-100 hover:text-gray-900 dark:hover:bg-primary-100/5 dark:hover:text-gray-50" type=button aria-label=语言><div class="flex items-center gap-2 capitalize"><svg height="18" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m10.5 21 5.25-11.25L21 21m-9-3h7.5M3 5.621a48.474 48.474.0 016-.371m0 0c1.12.0 2.233.038 3.334.114M9 5.25V3m3.334 2.364C11.176 10.658 7.69 15.08 3 17.502m9.334-12.138A47.63 47.63.0 0115 5.621m-4.589 8.495A18.023 18.023.0 016.584 8.314"/></svg><span>中文 (简体)</span></div></button><ul class="fixed m-0 min-w-[100px] hidden z-20 max-h-64 overflow-auto rounded-md ring-1 ring-black/5 bg-white py-1 text-sm shadow-lg dark:ring-white/20 dark:bg-neutral-800" style="inset:auto auto 0 0"><li class="flex flex-col"><a href=/en/research/llm/ class="relative cursor-pointer text-gray-800 dark:text-gray-100 hover:bg-primary-50 hover:text-primary-600 hover:dark:bg-primary-500/10 hover:dark:text-primary-200 whitespace-nowrap py-1.5 transition-colors ltr:pl-3 ltr:pr-9 rtl:pr-3 rtl:pl-9">English</a></li></ul></div></div></div></nav></header><div id=search class="hidden p-3 bg-light dark:bg-dark"></div></div><div class=page-body><section id=members class="relative hbb-section blox-collection" style="padding:1rem 0"><div class=home-section-bg></div><div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6 md:px-0"><div class="prose prose-slate dark:prose-invert max-w-prose"><h3 class=font-bold>团队成员</h3></div></div><div class="flex flex-col items-center px-6"><div class="container px-8 mx-auto xl:px-5 py-5 lg:py-8 max-w-[500px] justify-center"><div class="grid gap-10 md:grid-cols-1 lg:gap-10"><div class="resume-biography flex justify-center items-center flex-col"><div class="avatar-wrapper mt-5"><img class="avatar rounded-full bg-white dark:bg-gray-800 p-1" src=/author/%E9%BB%84%E4%B9%A6%E5%89%91/avatar_hu16618540271794702940.png alt=黄书剑 width=150 height=150></div><div class="portrait-title dark:text-white" style=text-align:center><div class="text-2xl font-bold mb-2 mt-6"><a href=http://nlp.nju.edu.cn/huangsj target=_blank rel=noopener>黄书剑</a></div><h3 class="font-semibold mb-1"><a href=https://cs.nju.edu.cn>南京大学  计算机学院</a></h3><div class=mb-2><a href=http://nlp.nju.edu.cn target=_blank rel=noopener><div>自然语言处理研究组</div></a></div></div><ul class="network-icon dark:text-zinc-100"><li class="mt-1 mb-1"><a href=mailto:huangsj@nju.edu.cn aria-label=at-symbol data-toggle=tooltip data-placement=top title="E-mail Me"><svg style="height:1.5rem" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5.0 11-9 0 4.5 4.5.0 019 0zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 10-2.636 6.364M16.5 12V8.25"/></svg></a></li></ul></div></div></div></div></section><section id=section-markdown class="relative hbb-section blox-markdown" style="padding:1rem 0"><div class=home-section-bg></div><div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6"><div class="prose prose-slate dark:prose-invert max-w-prose"><h3 class="mb-6 font-bold" style=margin-bottom:1.5rem>重要成果简介</h3></div><div class="prose prose-slate dark:prose-invert max-w-prose"><p>南京大学自然语言处理研究组长期从事自然语言处理、机器翻译等相关方向研究。在大模型时代，主要关注模型的知识、推理和多语言能力等方面的提升，开展了大语言模型的知识学习，大语言模型的翻译能力评估和激发、大语言模型能力的跨语言迁移以及多语言知识能力对齐等研究。</p><h3 id=代表性成果1llm的知识学习偏好>代表性成果1：LLM的知识学习偏好</h3><p><figure><div class="flex justify-center"><div class=w-100><img alt srcset="/research/llm/sec_1_figure_1_hu662749594090062069.webp 400w,
/research/llm/sec_1_figure_1_hu796553013621181017.webp 760w,
/research/llm/sec_1_figure_1_hu14147391922473614728.webp 1200w" src=/research/llm/sec_1_figure_1_hu662749594090062069.webp width=760 height=181 loading=lazy data-zoomable></div></div></figure></p><p>LLM如何学习包含冲突知识的数据是一个值得深入研究的科学问题。该工作通过在合成知识上的实验，揭示了大语言模型面临冲突知识时，更加倾向于偏好正式的、拼写正确的文本。进一步分析发现，包含特定特征的文本与其他数据的一致性程度是决定模型学习偏好程度的关键因素。一致性越高，模型对该特征的偏好越强。通过调整不同特征的知识一致性程度，我们可以为模型注入新的知识学习偏好，并可以消除甚至反转模型中现存的偏好。该工作获得EMNLP2024 Outstanding Paper Award。</p><h4 id=相关论文>相关论文：</h4><div>Jiahuan Li, Yiqing Cao, Shujian Huang*, Jiajun Chen. <a href=https://aclanthology.org/2024.emnlp-main.304v3.pdf>Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge</a>. EMNLP2024.<div class="flex flex-wrap space-x-3"><a href=https://www.bilibili.com/video/BV1khSKYSEKT target=_blank><img src=/images/bilibili.svg class=inline-block style=height:1.5em></img></a>
<a href=https://github.com/CaoYiqingT/Formality-is-Favored target=_blank><img src=/images/github.svg class=inline-block style=height:1.5em></img></a>
<a href="https://mp.weixin.qq.com/s/-T_J_QgqGZAPjcTRA938iA?token=36930520&lang=zh_CN" target=_blank><img src=/images/wechat.svg class=inline-block style=height:1.6em></img></a></div></div><h3 id=代表性成果2大语言模型的翻译能力评估和激发>代表性成果2：大语言模型的翻译能力评估和激发</h3><div class="flex justify-center"><img src=sec_2_figure_1.png style=max-width:500px;width:100%></div><p>该方面工作系统地评测了包括ChatGPT在内的一系列大语言模型在102种语言，202个以英文为核心的翻译方向上的多语言机器翻译能力，探究了使用大语言模型进行多语言机器翻译的优势与挑战。研究发现：即使是最强的大语言模型（ChatGPT），仍然在83.33%的翻译方向上落后于强大的有监督基线模型（NLLB）。经过进一步的分析实验，我们发现在机器翻译任务上，大语言模型展现出了一些新的工作模式。为后续大语言模型和机器翻译、多语言相关研究探索了方向。该工作两年内Google scholar被引293次。该研究还探索了基于多语言指令学习激发大语言模型翻译能力的可行性，并在能力激发的原理和泛化性等方面进行了探索。</p><h4 id=相关论文-1>相关论文：</h4><div>Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang*, Lingpeng Kong, Jiajun Chen, Lei Li. <a href=https://aclanthology.org/2024.findings-naacl.176.pdf>Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. </a>Findings of NAACL 2024.<div class="flex flex-wrap space-x-3"><a href=https://www.bilibili.com/video/BV16h411j7nW target=_blank><img src=/images/bilibili.svg class=inline-block style=height:1.5em></img></a>
<a href=https://github.com/NJUNLP/MMT-LLM target=_blank><img src=/images/github.svg class=inline-block style=height:1.5em></img></a>
<a href="https://mp.weixin.qq.com/s/AR5Yo14AzeVQuAgPwBsgrg?token=36930520&lang=zh_CN" target=_blank><img src=/images/wechat.svg class=inline-block style=height:1.6em></img></a></div></div><div>Jiahuan Li, Hao Zhou, Shujian Huang*, Shanbo Cheng, Jiajun Chen. <a href=https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00655/2367429/tacl_a_00655.pdf>Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions. </a>TACL 2024.<div class="flex flex-wrap space-x-3"><a href=https://github.com/NJUNLP/MFTI target=_blank><img src=/images/github.svg class=inline-block style=height:1.5em></img></a></div></div><h3 id=代表性成果3大语言模型能力的跨语言迁移>代表性成果3：大语言模型能力的跨语言迁移</h3><p><figure><div class="flex justify-center"><div class=w-100><img alt="LLM Transfer Figure 1" srcset="/research/llm/sec_3_figure_1_hu1060847605541939589.webp 400w,
/research/llm/sec_3_figure_1_hu6341551304818408691.webp 760w,
/research/llm/sec_3_figure_1_hu15106175007990104643.webp 1200w" src=/research/llm/sec_3_figure_1_hu1060847605541939589.webp width=760 height=353 loading=lazy data-zoomable></div></div></figure><figure><div class="flex justify-center"><div class=w-100><img alt="LLM Transfer Figure 2" srcset="/research/llm/sec_3_figure_2_hu7474879778897104707.webp 400w,
/research/llm/sec_3_figure_2_hu5600244559727793416.webp 760w,
/research/llm/sec_3_figure_2_hu8705114971244618110.webp 1200w" src=/research/llm/sec_3_figure_2_hu7474879778897104707.webp width=760 height=485 loading=lazy data-zoomable></div></div></figure></p><p>大语言模型的英文能力远高于其他语言。如何利用英文能力完成其他语言的任务，提升大语言模型在其他语言上的表现是一大挑战。我们尝试利用英文完成其他语言的任务（QAlign） 或者利用英文教会其他语言完成任务（MAPO），结果显示非英语可以取得大幅提升，缩小与英文差距。该工作受到Meta FAIR关注，在4月和11月的论文中引用我们的工作作为多语言偏好优化的代表工作。</p><h4 id=相关论文-2>相关论文：</h4><div>Wenhao Zhu, Shujian Huang*, Fei Yuan, Shuaijie She, Jiajun Chen, Alexandra Birch. <a href=https://aclanthology.org/2024.findings-acl.498.pdf>Question Translation Training for Better Multilingual Reasoning. </a>Findings of ACL 2024.<div class="flex flex-wrap space-x-3"><a href=https://www.bilibili.com/video/BV12E421w72W target=_blank><img src=/images/bilibili.svg class=inline-block style=height:1.5em></img></a>
<a href=https://github.com/NJUNLP/QAlign target=_blank><img src=/images/github.svg class=inline-block style=height:1.5em></img></a>
<a href="https://mp.weixin.qq.com/s/Z7qrtkpfyeDmbj0WKeBo_A?token=36930520&lang=zh_CN" target=_blank><img src=/images/wechat.svg class=inline-block style=height:1.6em></img></a></div></div><div>Shuaijie She, Wei Zou, Shujian Huang*, Wenhao Zhu, Xiang Liu, Xiang Geng, Jiajun Chen. <a href=https://aclanthology.org/2024.acl-long.539.pdf>MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization. </a>ACL 2024.<div class="flex flex-wrap space-x-3"><a href=https://www.bilibili.com/video/BV1j7421Z77h target=_blank><img src=/images/bilibili.svg class=inline-block style=height:1.5em></img></a>
<a href=https://github.com/NJUNLP/MAPO target=_blank><img src=/images/github.svg class=inline-block style=height:1.5em></img></a></div></div><h3 id=代表性成果4大语言模型知识和推理的跨语言传递>代表性成果4：大语言模型知识和推理的跨语言传递</h3><p><figure><div class="flex justify-center"><div class=w-100><img alt srcset="/research/llm/sec_4_figure_1_hu5771421348367609667.webp 400w,
/research/llm/sec_4_figure_1_hu6360803311509105053.webp 760w,
/research/llm/sec_4_figure_1_hu15045152543587388685.webp 1200w" src=/research/llm/sec_4_figure_1_hu5771421348367609667.webp width=760 height=296 loading=lazy data-zoomable></div></div></figure></p><p>当前的大型语言模型虽然的英语能力很强，在其他语言中表现的能力都相对较弱。但这些语言的能力与英文是否尊在跨语言传递关系尚不明确。该方面研究提出了一个系统框架CLiKA来评估LLM在性能、一致性和传导性方面的跨语言知识对齐，探讨了多语言预训练和指令调优对对齐程度的影响。研究发现：所有测试的LLM的整体跨语言知识对齐，尤其是在传导性层面，都不令人满意，多语言预训练和指令调优都不能显著提高跨语言知识传导性。该方面研究还进一步关注了推理类问题的化语言传递。研究发现：知识检索是影响推理能力跨语言传递的重要原因；现有大模型大多能进行知识无关推理的跨语言迁移，而在推理涉及模型自身包含的知识时，迁移能力受到验证影响。进一步探索大语言模型能力的跨语言传递将为探索更加公平的大语言模型研究和应用带来可能。</p><h4 id=相关论文-3>相关论文：</h4><div>Changjiang Gao, Hongda Hu, Peng Hu, Jiajun Chen, Jixing Li, Shujian Huang*. <a href=https://aclanthology.org/2024.naacl-long.339.pdf>Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge Alignment, But Only Shallowly.</a> NAACL2024.<div class="flex flex-wrap space-x-3"><a href=https://github.com/RiverGao/CLiKA target=_blank><img src=/images/github.svg class=inline-block style=height:1.5em></img></a></div></div><br><div>Peng Hu, Sizhe Liu, Changjiang Gao, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang*. <a href=https://arxiv.org/pdf/2406.16655>Large Language Models Are Cross-Lingual Knowledge-Free Reasoners.</a> NAACL 2025.<div class="flex flex-wrap space-x-3"><a href=https://github.com/NJUNLP/Knowledge-Free-Reasoning target=_blank><img src=/images/github.svg class=inline-block style=height:1.5em></img></a>
<a href="https://mp.weixin.qq.com/s?src=11&timestamp=1739071318&ver=5801&signature=VDex2DZdFia0F1gYOIEgTa8GVLBDWBbvxJxnE0NC2QNud3d8EI-g0ow10-DLlo8VOQYIjgfnVKdFLk318bLZ1erqgQl*MwZMsQi3xpj3F5qUGzIoLzNHphfx80b144T2&new=1" target=_blank><img src=/images/wechat.svg class=inline-block style=height:1.6em></img></a></div></div><h3 id=代表性成果5大语言模型的多语言预对齐>代表性成果5：大语言模型的多语言预对齐</h3><div class="flex flex-wrap justify-center"><img src=sec_5_figure_1.png style=width:100%;max-width:450px;object-fit:contain>
<img src=sec_5_figure_2.png style=width:100%;max-width:450px;object-fit:contain></div><p>大语言模型往往在以英语为中心的语料上进行训练，训练语料中其余语言数据只占很少的比例。尽管如此，现有的LLM仍然展现出了一定的多语言性能。这是因为LLM执行多语言任务的能力与模型多语言对齐能力（为平行文本生成相似的表示）正相关，而近来的研究表明，LLM可以自发形成一定程度的多语言对齐。然而，这种自发形成的对齐能力仍然相对较弱，这导致模型在跨语言知识检索和跨语言行为一致上仍然存在较大问题。该工作提出了预对齐（PreAlign）框架，通过将对齐建立的阶段提前到预训练之前，来更好地实现跨语言的迁移效果。为训练更加语言通用的大语言模型提供了一种可行方案。</p><h4 id=相关论文-4>相关论文：</h4><div>Jiahuan Li, Shujian Huang*, Xinyu Dai, Jiajun Chen. <a href=https://aclanthology.org/2024.emnlp-main.572v3.pdf>PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment.</a> EMNLP2024.<div class="flex flex-wrap space-x-3"><a href=https://www.bilibili.com/video/BV1khSKYSEKT target=_blank><img src=/images/bilibili.svg class=inline-block style=height:1.5em></img></a>
<a href=https://github.com/NJUNLP/PreAlign target=_blank><img src=/images/github.svg class=inline-block style=height:1.5em></img></a>
<a href="https://mp.weixin.qq.com/s/G72c4N02s8r2u3vpWI5BvQ?token=36930520&lang=zh_CN" target=_blank><img src=/images/wechat.svg class=inline-block style=height:1.6em></img></a></div></div></div></div></section></div><div class=page-footer><footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200"><div class="mx-auto flex gap-3 py-2 px-4"><div class=font-bold><svg class="inline-block pr-1" style="height:1em" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M12 21a9.004 9.004.0 008.716-6.747M12 21a9.004 9.004.0 01-8.716-6.747M12 21c2.485.0 4.5-4.03 4.5-9S14.485 3 12 3m0 18c-2.485.0-4.5-4.03-4.5-9S9.515 3 12 3m0 0a8.997 8.997.0 017.843 4.582M12 3A8.997 8.997.0 004.157 7.582m15.686.0A11.953 11.953.0 0112 10.5c-2.998.0-5.74-1.1-7.843-2.918m15.686.0A8.959 8.959.0 0121 12c0 .778-.099 1.533-.284 2.253m0 0A17.919 17.919.0 0112 16.5a17.92 17.92.0 01-8.716-2.247m0 0A9.015 9.015.0 013 12c0-1.605.42-3.113 1.157-4.418"/></svg>语言:</div><div class=font-bold>中文 (简体)</div><div><a href=/en/research/llm/>English</a></div></div><p class="powered-by text-center">© 2025 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class="powered-by text-center">由<a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a>支持发布——免费<a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>开源</a>网站，为创作者赋能。</p></footer></div><script src=https://cdn.staticfile.net/typed.js/2.1.0/typed.umd.min.js></script><script>var typed,element=document.getElementById("typed"),text=element?element.innerText:"";element.innerHTML="",typed=new Typed("#typed",{strings:[text],typeSpeed:100,showCursor:!1})</script></body></html>