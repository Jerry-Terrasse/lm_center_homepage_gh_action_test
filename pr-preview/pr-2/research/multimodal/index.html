<!doctype html><html lang=zh dir=ltr data-wc-theme-default=system><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 0.3.1"><meta name=author content="大模型研究协同中心"><meta name=description content="多模态大模型研究小组致力于推动多模态大模型的研究与应用，探索多模态信息融合、交互与推理等关键技术，推动多模态大模型在视觉、语音、文本等多模态数据上的应用，为多模态智能技术的发展提供技术支撑。
"><link rel=alternate hreflang=en href=/en/research/multimodal/><link rel=alternate hreflang=zh href=/research/multimodal/><link rel=stylesheet href=/css/themes/emerald.min.css><link href=/dist/wc.min.css rel=stylesheet><link href=/css/custom.min.97983a506d4b89401c7dc53cfb95c5059b49f13efdb74d01c3d038d88c173bcc.css rel=stylesheet><script>window.hbb={defaultTheme:document.documentElement.dataset.wcThemeDefault,setDarkTheme:()=>{document.documentElement.classList.add("dark"),document.documentElement.style.colorScheme="dark"},setLightTheme:()=>{document.documentElement.classList.remove("dark"),document.documentElement.style.colorScheme="light"}},console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`),"wc-color-theme"in localStorage?localStorage.getItem("wc-color-theme")==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme():(window.hbb.defaultTheme==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme(),window.hbb.defaultTheme==="system"&&(window.matchMedia("(prefers-color-scheme: dark)").matches?window.hbb.setDarkTheme():window.hbb.setLightTheme()))</script><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll("li input[type='checkbox'][disabled]");e.forEach(e=>{e.parentElement.parentElement.classList.add("task-list")});const t=document.querySelectorAll(".task-list li");t.forEach(e=>{let t=Array.from(e.childNodes).filter(e=>e.nodeType===3&&e.textContent.trim().length>1);if(t.length>0){const n=document.createElement("label");t[0].after(n),n.appendChild(e.querySelector("input[type='checkbox']")),n.appendChild(t[0])}})})</script><link rel=icon type=image/png href=/media/icon_hu3247630877640252165.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu4166356570829923896.png><link rel=canonical href=/research/multimodal/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@GetResearchDev"><meta property="twitter:creator" content="@GetResearchDev"><meta property="og:site_name" content="Hugo Academic CV Theme"><meta property="og:url" content="/research/multimodal/"><meta property="og:title" content="多模态大模型研究小组 | Hugo Academic CV Theme"><meta property="og:description" content="多模态大模型研究小组致力于推动多模态大模型的研究与应用，探索多模态信息融合、交互与推理等关键技术，推动多模态大模型在视觉、语音、文本等多模态数据上的应用，为多模态智能技术的发展提供技术支撑。"><meta property="og:image" content="/research/multimodal/featured.jpg"><meta property="twitter:image" content="/research/multimodal/featured.jpg"><meta property="og:locale" content="zh"><meta property="article:published_time" content="1020-01-01T00:00:00+00:00"><meta property="article:modified_time" content="1020-01-01T00:00:00+00:00"><title>多模态大模型研究小组 | Hugo Academic CV Theme</title><style>@font-face{font-family:inter var;font-style:normal;font-weight:100 900;font-display:swap;src:url(/dist/font/Inter.var.woff2)format(woff2)}</style><link type=text/css rel=stylesheet href=/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css integrity="sha256-vnZutBkxehTsdp0hbpd5v+jzc3yA54D0ug2vtXpBpII="><script src=/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script><script>window.hbb.pagefind={baseUrl:"/"}</script><style>html.dark{--pagefind-ui-primary:#eeeeee;--pagefind-ui-text:#eeeeee;--pagefind-ui-background:#152028;--pagefind-ui-border:#152028;--pagefind-ui-tag:#152028}</style><script>window.addEventListener("DOMContentLoaded",e=>{new PagefindUI({element:"#search",showSubResults:!0,baseUrl:window.hbb.pagefind.baseUrl,bundlePath:window.hbb.pagefind.baseUrl+"pagefind/"})}),document.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("search"),t=document.getElementById("search_toggle");t&&t.addEventListener("click",()=>{if(e.classList.toggle("hidden"),e.querySelector("input").value="",e.querySelector("input").focus(),!e.classList.contains("hidden")){let t=document.querySelector(".pagefind-ui__search-clear");t&&!t.hasAttribute("listenerOnClick")&&(t.setAttribute("listenerOnClick","true"),t.addEventListener("click",()=>{e.classList.toggle("hidden")}))}})})</script><link type=text/css rel=stylesheet href=/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM="><script defer src=/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js integrity="sha256-3ISyluw+iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script><script defer src=/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script><script defer src=/js/hugo-blox-zh.min.fd6dff43d6d3c4b67a94933185c6049c55f73fec192e44b3fe29bcdce8cd747a.js integrity="sha256-/W3/Q9bTxLZ6lJMxhcYEnFX3P+wZLkSz/im83OjNdHo="></script><script async defer src=https://buttons.github.io/buttons.js></script></head><body class="dark:bg-hb-dark dark:text-white page-wrapper" id=top><div id=page-bg></div><div class="page-header sticky top-0 z-30"><header id=site-header class=header><nav class="navbar px-3 flex justify-left"><div class="order-0 h-100"><a class=navbar-brand href=/ title="Hugo Academic CV Theme"><img fetchpriority=high decoding=async width=36 height=36 src=/media/icons/nju-emblem-cs_hu2207772332457637832.webp alt="Hugo Academic CV Theme">
<span class=max-lg:hidden>大模型研究协同创新中心</span></a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Open Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Close Menu</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left"><li class=nav-item><a class=nav-link href=/>主页</a></li><li class=nav-item><a class=nav-link href=/post>新闻</a></li><li class=nav-item><a class=nav-link href=/research>研究小组</a></li><li class=nav-item><a class=nav-link href=/publication/>出版物</a></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="text-black hover:text-primary inline-block px-3 text-xl dark:text-white" id=search_toggle><svg height="16" width="16" viewBox="0 0 512 512" fill="currentcolor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg></button><div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><button class="theme-toggle mt-1" accesskey=t title=appearance><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:hidden"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:block [&:not(dark)]:hidden"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div><div class="pl-1 mr-5 text-gray-600 hover:text-gray-800 dark:text-gray-400 dark:hover:text-gray-200
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><div class="flex justify-items-start"><button title=语言 data-state=closed data-hb-language-chooser class="grow h-7 rounded-md px-2 text-left text-xs font-medium text-gray-600 transition-colors dark:text-gray-400 hover:bg-gray-100 hover:text-gray-900 dark:hover:bg-primary-100/5 dark:hover:text-gray-50" type=button aria-label=语言><div class="flex items-center gap-2 capitalize"><svg height="18" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m10.5 21 5.25-11.25L21 21m-9-3h7.5M3 5.621a48.474 48.474.0 016-.371m0 0c1.12.0 2.233.038 3.334.114M9 5.25V3m3.334 2.364C11.176 10.658 7.69 15.08 3 17.502m9.334-12.138A47.63 47.63.0 0115 5.621m-4.589 8.495A18.023 18.023.0 016.584 8.314"/></svg><span>中文 (简体)</span></div></button><ul class="fixed m-0 min-w-[100px] hidden z-20 max-h-64 overflow-auto rounded-md ring-1 ring-black/5 bg-white py-1 text-sm shadow-lg dark:ring-white/20 dark:bg-neutral-800" style="inset:auto auto 0 0"><li class="flex flex-col"><a href=/en/research/multimodal/ class="relative cursor-pointer text-gray-800 dark:text-gray-100 hover:bg-primary-50 hover:text-primary-600 hover:dark:bg-primary-500/10 hover:dark:text-primary-200 whitespace-nowrap py-1.5 transition-colors ltr:pl-3 ltr:pr-9 rtl:pr-3 rtl:pl-9">English</a></li></ul></div></div></div></nav></header><div id=search class="hidden p-3 bg-light dark:bg-dark"></div></div><div class=page-body><section id=section-markdown class="relative hbb-section blox-markdown" style="padding:1rem 0"><div class=home-section-bg></div><div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6"><div class="prose prose-slate dark:prose-invert max-w-prose"><h3 class="mb-6 font-bold" style=margin-bottom:1.5rem></h3></div><div class="prose prose-slate dark:prose-invert max-w-prose"><h2 id=多模态大模型研究小组>多模态大模型研究小组</h2></div></div></section><section id=members class="relative hbb-section blox-collection" style="padding:1rem 0"><div class=home-section-bg></div><div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6 md:px-0"><div class="prose prose-slate dark:prose-invert max-w-prose"><h3 class=font-bold>团队成员</h3></div></div><div class="flex flex-col items-center px-6"><div class="container px-8 mx-auto xl:px-5 py-5 lg:py-8 max-w-screen-lg"><div class="grid gap-10 md:grid-cols-2 lg:gap-10"><div class="resume-biography flex justify-center items-center flex-col"><div class="avatar-wrapper mt-5"><img class="avatar rounded-full bg-white dark:bg-gray-800 p-1" src=/author/%E8%B7%AF%E9%80%9A/avatar_hu3774412003574957585.png alt=路通 width=150 height=150></div><div class="portrait-title dark:text-white" style=text-align:center><div class="text-2xl font-bold mb-2 mt-6"><a href=https://cs.nju.edu.cn/lutong/ target=_blank rel=noopener>路通</a></div><h3 class="font-semibold mb-1"><a href=https://www.nju.edu.cn>南京大学</a></h3><div class=mb-2><a href=https://cs.nju.edu.cn target=_blank rel=noopener><div>计算机学院</div></a></div></div><ul class="network-icon dark:text-zinc-100"><li class="mt-1 mb-1"><a href=mailto:lutong@nju.edu.cn aria-label=at-symbol data-toggle=tooltip data-placement=top title="E-mail Me"><svg style="height:1.5rem" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5.0 11-9 0 4.5 4.5.0 019 0zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 10-2.636 6.364M16.5 12V8.25"/></svg></a></li></ul></div><div class="resume-biography flex justify-center items-center flex-col"><div class="avatar-wrapper mt-5"><img class="avatar rounded-full bg-white dark:bg-gray-800 p-1" src=/author/%E7%8E%8B%E5%88%A9%E6%B0%91/avatar_hu15346742042315498946.png alt=王利民 width=150 height=150></div><div class="portrait-title dark:text-white" style=text-align:center><div class="text-2xl font-bold mb-2 mt-6"><a href=https://wanglimin.github.io/ target=_blank rel=noopener>王利民</a></div><h3 class="font-semibold mb-1"><a href=https://www.nju.edu.cn>南京大学</a></h3><div class=mb-2><a href=https://cs.nju.edu.cn target=_blank rel=noopener><div>计算机学院</div></a></div></div><ul class="network-icon dark:text-zinc-100"><li class="mt-1 mb-1"><a href=mailto:lmwang@nju.edu.cn aria-label=at-symbol data-toggle=tooltip data-placement=top title="E-mail Me"><svg style="height:1.5rem" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5.0 11-9 0 4.5 4.5.0 019 0zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 10-2.636 6.364M16.5 12V8.25"/></svg></a></li></ul></div></div></div></div></section><section id=section-markdown class="relative hbb-section blox-markdown" style="padding:1rem 0"><div class=home-section-bg></div><div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6"><div class="prose prose-slate dark:prose-invert max-w-prose"><h3 class="mb-6 font-bold" style=margin-bottom:1.5rem>重要成果简介</h3></div><div class="prose prose-slate dark:prose-invert max-w-prose"><p>南京大学与上海人工智能实验室等其他单位，共同研发了系列视频大模型、多模态大模型，在以视觉为中心的多模态理解任务上取得了世界领先性能。代表性成果如下：</p><h3 id=代表性成果1书生多模态大模型-internvl系列>代表性成果1：书生多模态大模型-InternVL系列</h3><p><figure><div class="flex justify-center"><div class=w-100><img src=./internvl.png alt=InternVL loading=lazy data-zoomable></div></div></figure></p><p>大型语言模型 (LLM) 的迅猛增长为多模态 AGI 系统开辟了无数可能性。然而，视觉和视觉语言基础模型（也是多模态 AGI 的关键要素）的进步却未能跟上 LLM 的步伐。在这项工作中，我们设计了一个大型视觉语言基础模型 (InternVL)，该模型将视觉基础模型扩展到 60 亿个参数，并逐步将其与 LLM 对齐，使用来自各种来源的网络规模图像文本数据。该模型可广泛应用于 32 个通用视觉语言基准测试，并在这些基准测试中取得最佳性能，包括视觉感知任务（例如图像级或像素级识别）、视觉语言任务（例如零样本图像/视频分类、零样本图像/视频文本检索），并与 LLM 链接以创建多模态对话系统。它具有强大的视觉功能，可以成为 ViT-22B 的良好替代品。</p><p>相关论文：</p><p>Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai, <a href=https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_InternVL_Scaling_up_Vision_Foundation_Models_and_Aligning_for_Generic_CVPR_2024_paper.pdf target=_blank>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</a>, in CVPR 2024.</p><h3 id=代表性成果2视频基础表征模型-videomae系列>代表性成果2：视频基础表征模型-VideoMAE系列</h3><p><figure><div class="flex justify-center"><div class=w-100><img src=./videomae.png alt=VideoMAE loading=lazy data-zoomable></div></div></figure></p><p>本成果提出了视频掩码自编码器 VideoMAE v1 & v2，成功训练出了首个十亿参数量的视频 Transformer 模型，突破了视频自监督表征学习的性能瓶颈。VideoMAE 系列工作引用超过了 1500 次，并且成为视频自监督学习领域的基准方法，被牛津大学、微软、谷歌、Meta 等单位进行了跟踪拓展研究，成为被开源社区 Hugging Face 收录的首个视频 Transformer 模型，全球调用下载量超过 320 万次，位列 Hugging Face 视频识别模型下载量榜首。</p><p>相关论文：</p><ul><li><p>Zhan Tong, Yibing Song, Jue Wang, Limin Wang, <a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/416f9cb3276121c42eebb86352a4354a-Paper-Conference.pdf target=_blank>VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training</a>, in NeurIPS 2022.</p></li><li><p>Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, Yu Qiao, <a href=https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.pdf target=_blank>VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking</a>, in CVPR 2023.</p></li></ul><h3 id=代表性成果3视频多模态大模型-videochat系列>代表性成果3：视频多模态大模型-VideoChat系列</h3><p><figure><div class="flex justify-center"><div class=w-100><img src=./videochat.png alt=VideoChat loading=lazy data-zoomable></div></div></figure></p><p>本成果提出了全球首个以视频为中心的多模态对话大模型 VideoChat，形成了对话驱动的通用视频理解能力，在重要多模态视频理解数据集上面取得了领先性能。VideoChat 相关技术被应用到快手可灵大模型的研发工作，GitHub 星标超过 3000。最近提出了 VideoChat-Online 和 VideoChat-Flash 版本，从交互形式和高效长时建模方面，进一步提升了 VideoChat 综合性能。</p><p>相关论文：</p><ul><li><p>Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao, <a href=https://arxiv.org/pdf/2305.06355 target=_blank>VideoChat: Chat-Centric Video Understanding</a>, arXiv:2305.06355</p></li><li><p>Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao, <a href=https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.pdf target=_blank>MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</a>, in CVPR 2024.</p></li><li><p>Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang, <a href=https://arxiv.org/pdf/2410.19702 target=_blank>TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning</a>, in ICLR 2025</p></li><li><p>Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, Limin Wang, <a href=https://arxiv.org/pdf/2501.00574 target=_blank>VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling</a>, arXiv: :2501.00574</p></li></ul><h3 id=代表性成果4书生视频大模型-internvideo系列>代表性成果4：书生视频大模型-InternVideo系列</h3><p><figure><div class="flex justify-center"><div class=w-100><img src=./internvideo.png alt=InternVideo loading=lazy data-zoomable></div></div></figure></p><p>本成果提出了全球首个通用视频理解大模型 InternVideo：在 2022 年发布了视频基础表征版本 InternVideo 1.0，在视频基础感知、视频时空解析、视频开发识别等重点任务取得了世界领先水平；在 2024 年发布了视频多模态理解版本 InternVideo 2.0，在超过 60 种视频理解任务上面取得领先性能，包括识别检索、开放问答、高阶推理等等；在 2025 年发布了深层次视频时空理解版本 InternVideo 2.5，在视频理解跨度和粒度上实现了显著性能提升，模型“记忆力”更是较前代提升了 6 倍。</p><p>相关论文：</p><ul><li><p>Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, Yu Qiao, <a href=https://arxiv.org/pdf/2212.03191 target=_blank>InternVideo: General Video Foundation Models via Generative and Discriminative Learning</a>, arXiv:2212.03191</p></li><li><p>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang, <a href=https://arxiv.org/pdf/2403.15377 target=_blank>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</a>, in ECCV 2024.</p></li><li><p>Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, Limin Wang, <a href=https://arxiv.org/pdf/2501.12386 target=_blank>InternVideo2. 5: Empowering Video MLLMs with Long and Rich Context Modeling</a>, arXiv: 2501.12386</p></li></ul></div></div></section></div><div class=page-footer><footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200"><div class="mx-auto flex gap-3 py-2 px-4"><div class=font-bold><svg class="inline-block pr-1" style="height:1em" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M12 21a9.004 9.004.0 008.716-6.747M12 21a9.004 9.004.0 01-8.716-6.747M12 21c2.485.0 4.5-4.03 4.5-9S14.485 3 12 3m0 18c-2.485.0-4.5-4.03-4.5-9S9.515 3 12 3m0 0a8.997 8.997.0 017.843 4.582M12 3A8.997 8.997.0 004.157 7.582m15.686.0A11.953 11.953.0 0112 10.5c-2.998.0-5.74-1.1-7.843-2.918m15.686.0A8.959 8.959.0 0121 12c0 .778-.099 1.533-.284 2.253m0 0A17.919 17.919.0 0112 16.5a17.92 17.92.0 01-8.716-2.247m0 0A9.015 9.015.0 013 12c0-1.605.42-3.113 1.157-4.418"/></svg>语言:</div><div class=font-bold>中文 (简体)</div><div><a href=/en/research/multimodal/>English</a></div></div><p class="powered-by text-center">© 2025 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class="powered-by text-center">由<a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a>支持发布——免费<a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>开源</a>网站，为创作者赋能。</p></footer></div><script src=https://cdn.staticfile.net/typed.js/2.1.0/typed.umd.min.js></script><script>var typed,element=document.getElementById("typed"),text=element?element.innerText:"";element.innerHTML="",typed=new Typed("#typed",{strings:[text],typeSpeed:100,showCursor:!1})</script></body></html>